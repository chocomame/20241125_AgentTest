import streamlit as st
import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
from urllib.parse import urljoin, urlparse, unquote, parse_qs
import pandas as pd
import re
from html.parser import HTMLParser

class HTMLSyntaxChecker(HTMLParser):
    def __init__(self):
        super().__init__()
        self.errors = []
        self.open_tags = []
        self.line_number = 1
        self.column = 0

    def handle_starttag(self, tag, attrs):
        self.open_tags.append((tag, self.getpos()))

    def handle_endtag(self, tag):
        if self.open_tags:
            if self.open_tags[-1][0] == tag:
                self.open_tags.pop()
            else:
                pos = self.getpos()
                self.errors.append(f"ã‚¿ã‚°ã®é–‰ã˜å¿˜ã‚Œ: è¡Œ {pos[0]}, åˆ— {pos[1]} - {tag}ã‚¿ã‚°ãŒæ­£ã—ãé–‰ã˜ã‚‰ã‚Œã¦ã„ã¾ã›ã‚“")

    def handle_data(self, data):
        self.line_number += data.count('\n')

def check_html_syntax(html_content):
    """HTMLã®é–‰ã˜ã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯"""
    syntax_errors = []
    
    # PHPã‚³ãƒ¼ãƒ‰ã¨ç‰¹æ®Šæ–‡å­—ã‚’ä¸€æ™‚çš„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç½®æ›
    replacements = []
    
    def replace_special_content(content):
        # PHPã‚³ãƒ¼ãƒ‰ã‚’ç½®æ›
        content = re.sub(r'<\?php.*?\?>', 
                        lambda match: f"PLACEHOLDER_{len(replacements)}",
                        content, flags=re.DOTALL)
        
        # ç‰¹æ®Šæ–‡å­—ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ç½®æ›
        content = re.sub(r'&[a-zA-Z]+;',
                        lambda match: f"PLACEHOLDER_{len(replacements)}",
                        content)
        
        return content
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‰å‡¦ç†
    processed_content = replace_special_content(html_content)
    
    # HTMLã‚’ãƒ‘ãƒ¼ã‚¹
    soup = BeautifulSoup(processed_content, 'html.parser')
    
    # é‡è¦ãªã‚¿ã‚°ã®ãƒªã‚¹ãƒˆï¼ˆãƒã‚§ãƒƒã‚¯ã—ãŸã„ã‚¿ã‚°ã‚’æŒ‡å®šï¼‰
    important_tags = ['div', 'p', 'section', 'article', 'main', 'header', 'footer', 'nav', 'aside']
    
    # è¡Œç•ªå·ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®æº–å‚™
    lines = processed_content.split('\n')
    original_lines = html_content.split('\n')
    
    for tag_name in important_tags:
        # ã‚¿ã‚°ã®é–‹å§‹ä½ç½®ã‚’å…¨ã¦å–å¾—
        tag_pattern = re.compile(f'<{tag_name}[^>]*>')
        start_positions = [(i + 1, m.start()) for i, line in enumerate(lines) 
                          for m in re.finditer(tag_pattern, line)]
        
        # ã‚¿ã‚°ã®çµ‚äº†ä½ç½®ã‚’å…¨ã¦å–å¾—
        end_pattern = re.compile(f'</{tag_name}>')
        end_positions = [(i + 1, m.start()) for i, line in enumerate(lines) 
                        for m in re.finditer(end_pattern, line)]
        
        # é–‹å§‹ã‚¿ã‚°ã¨çµ‚äº†ã‚¿ã‚°ã®æ•°ã‚’æ¯”è¼ƒ
        if len(start_positions) > len(end_positions):
            # é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„ã‚¿ã‚°ã®ä½ç½®ã‚’ç‰¹å®š
            unclosed_tags = []
            for i, (line_num, pos) in enumerate(start_positions):
                # å¯¾å¿œã™ã‚‹çµ‚äº†ã‚¿ã‚°ãŒãªã„å ´åˆ
                if i >= len(end_positions):
                    # å…ƒã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    original_line = original_lines[line_num - 1]
                    context_start = max(0, pos - 25)
                    context_end = min(len(original_line), pos + 25)
                    context = original_line[context_start:context_end]
                    
                    # æ˜ã‚‰ã‹ãªèª¤æ¤œå‡ºã‚’é™¤å¤–
                    if not any(ignore in context for ignore in [
                        '<?php',
                        '?>',
                        '<!--',
                        '-->',
                        '[',
                        ']',
                        '&copy;'  # ç‰¹æ®Šæ–‡å­—ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é™¤å¤–
                    ]):
                        # è¡Œå…¨ä½“ã‚’ç¢ºèªã—ã¦é–‰ã˜ã‚¿ã‚°ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        full_line = original_line
                        if f'</{tag_name}>' in full_line:
                            continue  # åŒã˜è¡Œã«é–‰ã˜ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        
                        unclosed_tags.append(f"è¡Œ {line_num}: {context}")
            
            if unclosed_tags:
                syntax_errors.append(f"è­¦å‘Š: {tag_name}ã‚¿ã‚°ã®é–‰ã˜å¿˜ã‚ŒãŒ{len(unclosed_tags)}å€‹ã‚ã‚Šã¾ã™:")
                for location in unclosed_tags:
                    syntax_errors.append(f"  â†’ {location}")
    
    return syntax_errors

def is_same_domain(url, base_domain):
    """URLãŒåŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    return urlparse(url).netloc == base_domain

def is_anchor_link(url):
    """ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    return '#' in url

def normalize_url(url):
    """URLã‚’æ­£è¦åŒ–ï¼ˆæœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼‰"""
    # URLãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬èªãªã©ã‚’èª­ã‚ã‚‹å½¢å¼ã«ï¼‰
    decoded_url = unquote(url, encoding='utf-8')
    
    # index.htmlã‚’å«ã‚€URLã‚’ãƒ«ãƒ¼ãƒˆURLã«æ­£è¦åŒ–
    if decoded_url.lower().endswith('/index.html'):
        decoded_url = decoded_url[:-10]  # '/index.html'ã®é•·ã•(10)ã‚’å‰Šé™¤
    elif decoded_url.lower().endswith('index.html'):
        decoded_url = decoded_url[:-9]  # 'index.html'ã®é•·ã•(9)ã‚’å‰Šé™¤
    
    # .htmlã§çµ‚ã‚ã‚‹URLã®å ´åˆã¯ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
    if decoded_url.lower().endswith('.html'):
        return decoded_url
    # ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€æœ«å°¾ã«ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ 
    elif not decoded_url.endswith('/'):
        return decoded_url + '/'
    return decoded_url

def contains_japanese(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]')
    return bool(japanese_pattern.search(text))

def check_heading_order(soup):
    """è¦‹å‡ºã—ã‚¿ã‚°ã®é †åºã¨è‹±èªã®ã¿ã®è¦‹å‡ºã—ã‚’ãƒã‚§ãƒƒã‚¯"""
    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    heading_issues = []
    english_only_headings = []
    prev_level = 0
    
    for i, heading in enumerate(headings, 1):
        current_level = int(heading.name[1])
        heading_text = heading.get_text().strip()
        
        # è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if prev_level > 0:
            # è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ãŒ2ä»¥ä¸Šé£›ã‚“ã§ã„ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
            # h2â†’h4, h3â†’h5 ãªã©ã®é£›ã³ã‚’æ¤œå‡º
            if current_level - prev_level > 1 and current_level > prev_level:
                heading_issues.append(
                    f"h{prev_level}ã‹ã‚‰h{current_level}ã«é£›ã‚“ã§ã„ã¾ã™ï¼ˆ{heading_text}ï¼‰"
                )
        elif current_level != 1 and i == 1:
            # æœ€åˆã®è¦‹å‡ºã—ãŒh1ã§ãªã„å ´åˆ
            heading_issues.append(
                f"æœ€åˆã®è¦‹å‡ºã—ãŒh1ã§ã¯ãªãh{current_level}ã§ã™ï¼ˆ{heading_text}ï¼‰"
            )
        
        # è‹±èªã®ã¿ã®ãƒã‚§ãƒƒã‚¯
        if heading_text and not contains_japanese(heading_text):
            # æ•°å­—ã¨ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if re.match(r'^[a-zA-Z0-9\s\-_.,!?]*$', heading_text):
                english_only_headings.append(f"{heading.name}: {heading_text}")
        
        prev_level = current_level
    
    # å•é¡Œã‚’é€£ç•ªã§æ•´å½¢
    formatted_issues = []
    for i, issue in enumerate(heading_issues, 1):
        formatted_issues.append(f"{i}: {issue}")
    
    return formatted_issues, english_only_headings

def check_image_alt(soup, url):
    """ç”»åƒã®altå±æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    # /blog/ã¾ãŸã¯/category/ã‚’å«ã‚€URLã®å ´åˆã¯ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if '/blog/' in url or '/category/' in url:
        return 'skip'
        
    images_without_alt = []
    total_images = 0
    
    # ã™ã¹ã¦ã®imgè¦ç´ ã‚’æ¤œç´¢
    all_images = soup.find_all('img')
    
    for img in all_images:
        src = None
        # srcå±æ€§ã®ç¢ºèª
        if img.get('src'):
            src = img.get('src')
        # srcsetå±æ€§ã®ç¢ºèª
        elif img.get('srcset'):
            srcset = img.get('srcset')
            # æœ€åˆã®ç”»åƒURLã‚’å–å¾—
            src = srcset.split(',')[0].strip().split(' ')[0]
            
        if src:
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            if not src.startswith(('http://', 'https://')):
                src = urljoin(url, src)
            
            # ãƒ‡ãƒ¼ã‚¿URLã‚„base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–
            if not src.startswith('data:') and not src.lower().endswith('.pdf'):
                total_images += 1
                alt = img.get('alt', '').strip()
                if not alt:
                    if src not in images_without_alt:
                        images_without_alt.append(src)
    
    # çµæœã‚’è¿”ã™
    if total_images == 0:
        return '- ç”»åƒãªã—'
    elif images_without_alt:
        # URLã‚’HTMLå½¢å¼ã®ãƒªãƒ³ã‚¯ã«å¤‰æ›ã—ã€æ”¹è¡Œã‚’brã‚¿ã‚°ã«å¤‰æ›
        linked_urls = [f'<span class="image-url">{i+1}: <a href="{img_url}" target="_blank">{img_url}</a></span>' 
                      for i, img_url in enumerate(images_without_alt)]
        return f'âŒ altå±æ€§ãªã—:<br>' + ''.join(linked_urls)
    else:
        return 'âœ… OK'

def check_keyword_repetition(text):
    """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯"""
    if not text:
        return []
    
    # ç‰¹å®šã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆç„¡è¦–ã™ã‚‹å˜èªï¼‰ã‚’å®šç¾©
    stop_words = {'ã®', 'ã‚„', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã§', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã‚‚', 'ã¯', 'ãƒ»', '|', '-'}
    
    # è¨±ç™‚ç§‘é–¢é€£ã®ç”¨èªï¼ˆã“ã‚Œã‚‰ã¯é‡è¤‡ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ãªã„ï¼‰
    medical_specialties = {
        # åŸºæœ¬è¨ºç™‚ç§‘
        'å†…ç§‘', 'å¤–ç§‘', 'çœ¼ç§‘', 'æ­¯ç§‘', 'è€³é¼»ç§‘', 'çš®è†šç§‘', 'å°å…ç§‘',
        'æ•´å½¢å¤–ç§‘', 'ç”£å©¦äººç§‘', 'æ³Œå°¿å™¨ç§‘', 'ç²¾ç¥ç§‘', 'è„³ç¥çµŒå¤–ç§‘',
        'æ”¾å°„ç·šç§‘', 'éº»é…”ç§‘', 'å½¢æˆå¤–ç§‘', 'æ•‘æ€¥ç§‘',
        
        # æ­¯ç§‘ã®å°‚é–€åˆ†é‡
        'å°å…æ­¯ç§‘', 'çŸ¯æ­£æ­¯ç§‘', 'å¯©ç¾æ­¯ç§‘', 'å£è…”å¤–ç§‘', 'æ­¯ç§‘å£è…”å¤–ç§‘',
        'äºˆé˜²æ­¯ç§‘', 'ä¿å­˜æ­¯ç§‘', 'è£œç¶´æ­¯ç§‘', 'ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆ', 'ä¸€èˆ¬æ­¯ç§‘',
        
        # å†…ç§‘ã®å°‚é–€åˆ†é‡
        'æ¶ˆåŒ–å™¨å†…ç§‘', 'å¾ªç’°å™¨å†…ç§‘', 'å‘¼å¸å™¨å†…ç§‘', 'è„³ç¥çµŒå†…ç§‘',
        'è¡€æ¶²å†…ç§‘', 'è…è‡“å†…ç§‘', 'ç³–å°¿ç—…å†…ç§‘', 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ç§‘',
        
        # ãã®ä»–ã®å°‚é–€åˆ†é‡
        'æ¶ˆåŒ–å™¨å¤–ç§‘', 'å¿ƒè‡“è¡€ç®¡å¤–ç§‘', 'å‘¼å¸å™¨å¤–ç§‘', 'è„³ç¥çµŒå¤–ç§‘',
        'å°å…å¤–ç§‘', 'ä¹³è…ºå¤–ç§‘', 'æ°—ç®¡é£Ÿé“ç§‘',
        
        # åŒ»ç™‚æ©Ÿé–¢ã‚’è¡¨ã™ä¸€èˆ¬çš„ãªç”¨èª
        'ç—…é™¢', 'ã‚¯ãƒªãƒ‹ãƒƒã‚¯', 'åŒ»é™¢', 'è¨ºç™‚æ‰€'
    }
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†ã‹ã¡æ›¸ãã—ã¦å˜èªã«åˆ†å‰²
    words = []
    temp_words = re.findall(r'[ä¸€-é¾¯ã-ã‚“ã‚¡-ãƒ³a-zA-Z0-9]+', text)
    
    # è¤‡åˆèªã‚’å„ªå…ˆçš„ã«æ¤œå‡º
    i = 0
    while i < len(temp_words):
        # 3å˜èªã®çµ„ã¿åˆã‚ã›ã‚’ãƒã‚§ãƒƒã‚¯
        if i + 2 < len(temp_words):
            triple = temp_words[i] + temp_words[i + 1] + temp_words[i + 2]
            if triple in medical_specialties:
                words.append(triple)
                i += 3
                continue
        
        # 2å˜èªã®çµ„ã¿åˆã‚ã›ã‚’ãƒã‚§ãƒƒã‚¯
        if i + 1 < len(temp_words):
            double = temp_words[i] + temp_words[i + 1]
            if double in medical_specialties:
                words.append(double)
                i += 2
                continue
        
        # å˜èªãŒ2æ–‡å­—ä»¥ä¸Šã§ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã§ãªã„å ´åˆã®ã¿è¿½åŠ 
        if len(temp_words[i]) >= 2 and temp_words[i] not in stop_words:
            # è¨ºç™‚ç§‘é–¢é€£ã®ç”¨èªã‹ãƒã‚§ãƒƒã‚¯
            if temp_words[i] in medical_specialties:
                words.append(temp_words[i])
            else:
                # è¨ºç™‚ç§‘ä»¥å¤–ã®å˜èªã®ã¿ã‚«ã‚¦ãƒ³ãƒˆå¯¾è±¡ã¨ã™ã‚‹
                words.append(temp_words[i])
        i += 1
    
    # å„å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆè¨ºç™‚ç§‘é–¢é€£ã®ç”¨èªã¯é™¤å¤–ï¼‰
    word_count = {}
    for word in words:
        if word not in medical_specialties:  # è¨ºç™‚ç§‘é–¢é€£ã®ç”¨èªã¯å®Œå…¨ã«é™¤å¤–
            word_count[word] = word_count.get(word, 0) + 1
    
    # 3å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹å˜èªã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    repeated_words = [f"'{word}' ({count}å›)" for word, count in word_count.items() if count >= 3]
    
    return repeated_words

def get_page_info(url):
    """ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    try:
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URLã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if is_preview_url(url):
            return {
                'url': normalize_url(url),
                'title': "ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URLï¼‰",
                'description': "ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URLï¼‰",
                'title_length': 0,
                'description_length': 0,
                'title_status': '- ã‚¹ã‚­ãƒƒãƒ—',
                'description_status': '- ã‚¹ã‚­ãƒƒãƒ—',
                'heading_issues': '- ã‚¹ã‚­ãƒƒãƒ—',
                'english_only_headings': '- ã‚¹ã‚­ãƒƒãƒ—',
                'images_without_alt': '- ã‚¹ã‚­ãƒƒãƒ—',
                'html_syntax': '- ã‚¹ã‚­ãƒƒãƒ—'
            }

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è‡ªå‹•æ¤œå‡ºã¨è¨­å®š
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding
        
        # 404ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯çµæœã«å«ã‚ãªã„
        if response.status_code == 404:
            return None
        
        response.raise_for_status()
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸURLã‚’ä½¿ç”¨
        normalized_url = normalize_url(url)
        
        result = {
            'url': normalized_url,  # æ­£è¦åŒ–ã•ã‚ŒãŸURLã‚’ä½¿ç”¨
            'title': "å–å¾—ã‚¨ãƒ©ãƒ¼",
            'description': "å–å¾—ã‚¨ãƒ©ãƒ¼",
            'title_length': 0,
            'description_length': 0,
            'title_status': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'description_status': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'heading_issues': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'english_only_headings': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'images_without_alt': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'html_syntax': 'âŒ ã‚¨ãƒ©ãƒ¼'
        }
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
        try:
            title = soup.title.string.strip() if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            title_repetitions = check_keyword_repetition(title)
            title_status = []
            
            # é•·ã•ãƒã‚§ãƒƒã‚¯
            if len(title) > 50:
                title_status.append('âŒ é•·ã™ãã¾ã™ï¼ˆ50æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰')
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if title_repetitions:
                title_status.append(f'âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¤‡: {", ".join(title_repetitions)}')
            
            # å•é¡ŒãŒãªã„å ´åˆ
            if not title_status:
                title_status = ['âœ… OK']
            
            result.update({
                'title': title,
                'title_length': len(title),
                'title_status': '<br>'.join(title_status)
            })
        except Exception as e:
            st.error(f"ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®å–å¾—ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
        try:
            description = ""
            meta_desc = soup.find('meta', attrs={'name': re.compile('^[Dd]escription$')})
            if meta_desc:
                description = meta_desc.get('content', '').strip()
            if not description:
                og_desc = soup.find('meta', attrs={'property': 'og:description'})
                if og_desc:
                    description = og_desc.get('content', '').strip()
            
            description_repetitions = check_keyword_repetition(description)
            description_status = []
            
            # é•·ã•ãƒã‚§ãƒƒã‚¯
            if len(description) > 140:
                description_status.append('âŒ é•·ã™ãã¾ã™ï¼ˆ140æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰')
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if description_repetitions:
                description_status.append(f'âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¤‡: {", ".join(description_repetitions)}')
            
            # å•é¡ŒãŒãªã„å ´åˆ
            if not description_status:
                description_status = ['âœ… OK']
            
            result.update({
                'description': description,
                'description_length': len(description),
                'description_status': '<br>'.join(description_status)
            })
        except Exception as e:
            st.error(f"ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # è¦‹å‡ºã—ã‚¿ã‚°ã®ãƒã‚§ãƒƒã‚¯
        try:
            heading_issues, english_only_headings = check_heading_order(soup)
            # å•é¡ŒãŒã‚ã‚‹å ´åˆã¯å…¨ã¦ã®å•é¡Œã‚’è¡¨ç¤º
            if heading_issues:
                result['heading_issues'] = '<br>'.join(heading_issues)
            else:
                result['heading_issues'] = 'âœ… OK'
                
            if english_only_headings:
                result['english_only_headings'] = '<br>'.join(english_only_headings)
            else:
                result['english_only_headings'] = 'âœ… OK'
        except Exception as e:
            st.error(f"è¦‹å‡ºã—ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ç”»åƒã®altå±æ€§ãƒã‚§ãƒƒã‚¯
        try:
            alt_check_result = check_image_alt(soup, url)
            result['images_without_alt'] = alt_check_result
        except Exception as e:
            st.error(f"ç”»åƒãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
            result['images_without_alt'] = f'âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}'
        
        # HTMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        try:
            syntax_errors = check_html_syntax(html_content)
            result['html_syntax'] = '\n'.join(syntax_errors) if syntax_errors else 'âœ… OK'
        except Exception as e:
            st.error(f"HTMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return result
        
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            'url': normalize_url(url),
            'title': f'æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}',
            'description': 'æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'title_length': 0,
            'description_length': 0,
            'title_status': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'description_status': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'heading_issues': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'english_only_headings': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'images_without_alt': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼',
            'html_syntax': 'âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼'
        }
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            'url': normalize_url(url),
            'title': f'ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'description': 'ã‚¨ãƒ©ãƒ¼',
            'title_length': 0,
            'description_length': 0,
            'title_status': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'description_status': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'heading_issues': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'english_only_headings': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'images_without_alt': 'âŒ ã‚¨ãƒ©ãƒ¼',
            'html_syntax': 'âŒ ã‚¨ãƒ©ãƒ¼'
        }

def get_all_links(url, base_domain):
    """ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è‡ªå‹•æ¤œå‡ºã¨è¨­å®š
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding
        
        # 404ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
        if response.status_code == 404:
            return set()
            
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        links = set()
        # aã‚¿ã‚°ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¿ã‚°ã®ä¸¡æ–¹ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—
        for element in soup.find_all(['a', 'frame', 'iframe']):
            href = element.get('href') or element.get('src')
            if href:
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                absolute_url = urljoin(url, href)
                parsed_url = urlparse(absolute_url)
                
                # index.htmlã§çµ‚ã‚ã‚‹URLã¯é™¤å¤–ï¼ˆãƒ«ãƒ¼ãƒˆURLã¨åŒã˜ãŸã‚ï¼‰
                if parsed_url.path.lower().endswith('index.html'):
                    continue
                
                # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã®URLã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
                if (parsed_url.netloc == base_domain and
                    not is_anchor_link(absolute_url) and
                    not absolute_url.lower().endswith('.pdf') and
                    not is_preview_url(absolute_url)):
                    
                    # URLã‚’æ­£è¦åŒ–ï¼ˆæœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼‰
                    normalized_url = absolute_url.rstrip('/')
                    
                    # .htmlã§çµ‚ã‚ã‚‹URLã®å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
                    if normalized_url.lower().endswith('.html'):
                        links.add(normalized_url)
                    # .htmlã§çµ‚ã‚ã‚‰ãªã„URLã®å ´åˆã¯ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ 
                    else:
                        links.add(normalized_url + '/')
        
        return links
    except requests.exceptions.RequestException as e:
        if not isinstance(e, requests.exceptions.HTTPError) or e.response.status_code != 404:
            st.error(f"ãƒªãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return set()
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return set()

def is_preview_url(url):
    """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URLã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    preview_params = ['preview', 'preview_id', 'preview_nonce', '_thumbnail_id']
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    return any(param in query_params for param in preview_params)

def main():
    # ãƒšãƒ¼ã‚¸å¹…ã®è¨­å®š
    st.set_page_config(
        page_title="WEBã‚µã‚¤ãƒˆSEOãƒã‚§ãƒƒã‚«ãƒ¼",
        layout="wide",
        initial_sidebar_state="auto"
    )

    # ã‚¹ã‚¿ãƒ CSS
    st.markdown("""
        <style>
        .block-container {
            max-width: 1104px;  # 736px * 1.5
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("ğŸ” WEBã‚µã‚¤ãƒˆSEOãƒã‚§ãƒƒã‚«ãƒ¼")
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã¨å¤‰æ›´å±¥æ­´
    with st.expander("ğŸ“‹ ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã¨å¤‰æ›´å±¥æ­´"):
        st.write("""
        **ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v1.0.0** ğŸš€ (2024.11.26 ãƒªãƒªãƒ¼ã‚¹)
        
        **å¤‰æ›´å±¥æ­´ï¼š**
        - v1.0.0 (2024.11.26)
          - âœ¨ åˆå›ãƒªãƒªãƒ¼ã‚¹
          - ğŸ¯ åŸºæœ¬çš„ãªSEOè¦ç´ ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’å®Ÿè£…
        - v0.9.0 (2024.11.25)
          - ğŸ”§ ãƒ™ãƒ¼ã‚¿ç‰ˆãƒªãƒªãƒ¼ã‚¹
          - ğŸ§ª ãƒ†ã‚¹ãƒˆé‹ç”¨é–‹å§‹
        """)
    
    st.write("""
    ã“ã®ãƒ„ãƒ¼ãƒ«ã¯æŒ‡å®šã—ãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®SEOè¦ç´ ã‚’è‡ªå‹•çš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚

    ### ğŸ“Š ãƒã‚§ãƒƒã‚¯é …ç›®ï¼š
    1. ğŸ“ ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ï¼ˆæ–‡å­—æ•°ã¨å†…å®¹ã€50æ–‡å­—ä»¥å†…ï¼‰
    2. ğŸ“„ ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ–‡å­—æ•°ã¨å†…å®¹ã€140æ–‡å­—ä»¥å†…ï¼‰
    3. ğŸ“‘ è¦‹å‡ºã—æ§‹é€ ï¼ˆh1ã€œh6ã®éšå±¤é–¢ä¿‚ï¼‰
    4. ğŸ–¼ï¸ ç”»åƒã®altå±æ€§ï¼ˆä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆã®æœ‰ç„¡ï¼‰
    5. ğŸ”§ HTMLæ§‹æ–‡ï¼ˆé–‰ã˜ã‚¿ã‚°ã®æœ‰ç„¡ï¼‰ã®æ­£ç¢ºæ€§

    ### ğŸš€ ä½¿ã„æ–¹ï¼š
    1. ğŸ”— ãƒã‚§ãƒƒã‚¯ã—ãŸã„ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›
    2. â–¶ ã€Œãƒã‚§ãƒƒã‚¯é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
    3. âœ¨ è‡ªå‹•çš„ã«å…¨ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€çµæœã‚’è¡¨ç¤º
    """)
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    url = st.text_input("ğŸŒ ãƒã‚§ãƒƒã‚¯ã—ãŸã„WEBã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", "")
    
    if st.button("ğŸ” ãƒã‚§ãƒƒã‚¯é–‹å§‹") and url:
        base_domain = urlparse(url).netloc
        
        with st.spinner('ğŸ”„ ã‚µã‚¤ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ä¸­...'):
            # è¨ªå•æ¸ˆã¿URLã‚’ç®¡ç†
            visited_urls = set()
            urls_to_visit = {url}
            results = []
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®åˆæœŸåŒ–
            progress_bar = st.progress(0)
            
            while urls_to_visit:
                current_url = urls_to_visit.pop()
                normalized_current_url = normalize_url(current_url)
                
                if normalized_current_url not in visited_urls:
                    visited_urls.add(normalized_current_url)
                    
                    # ãƒšãƒ¼ã‚¸æƒ…å ±ã®å–å¾—
                    page_info = get_page_info(current_url)
                    # 404ã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®çµæœã®ã¿ã‚’è¿½åŠ 
                    if page_info is not None:
                        results.append(page_info)
                    
                    # æ–°ã—ã„ãƒªãƒ³ã‚¯ã®å–å¾—
                    new_links = get_all_links(current_url, base_domain)
                    urls_to_visit.update(new_links - visited_urls)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°
                progress = len(visited_urls) / (len(visited_urls) + len(urls_to_visit))
                progress_bar.progress(min(progress, 1.0))
            
            # çµæœã®è¡¨ç¤º
            if results:
                df = pd.DataFrame(results)
                st.write(f"âœ… ãƒã‚§ãƒƒã‚¯å®Œäº†ï¼ åˆè¨ˆ{len(results)}ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã€‚")
                
                # ã‚¿ãƒ–ã§çµæœã‚’è¡¨ç¤º
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ğŸ“ ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³",
                    "ğŸ“‘ è¦‹å‡ºã—æ§‹é€ ",
                    "ğŸ”¤ è‹±èªã®ã¿ã®è¦‹å‡ºã—",
                    "ğŸ–¼ï¸ ç”»åƒaltå±æ€§",
                    "ğŸ”§ HTMLæ§‹æ–‡"
                ])
                
                with tab1:
                    st.subheader("ğŸ“Š ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒã‚§ãƒƒã‚¯")
                    # HTMLã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¯èƒ½ã«ã™ã‚‹
                    st.markdown("""
                        <style>
                        .dataframe a {
                            color: #1E88E5;
                            text-decoration: underline;
                        }
                        .dataframe td {
                            max-width: 300px;
                            white-space: normal !important;
                            padding: 8px !important;
                            vertical-align: top;
                        }
                        .dataframe th {
                            padding: 8px !important;
                            background-color: #f8f9fa;
                        }
                        .status-ok {
                            color: #28a745;
                            font-weight: bold;
                        }
                        .status-error {
                            color: #dc3545;
                            font-weight: bold;
                        }
                        .length-info {
                            color: #6c757d;
                            font-size: 0.9em;
                        }
                        </style>
                    """, unsafe_allow_html=True)

                    # ãƒ‡ï¿½ï¿½ã‚’æ•´å½¢
                    display_df = df.copy()
                    # URLã®è¡¨ç¤ºã‚’ä¿®æ­£
                    display_df['url'] = display_df['url'].apply(
                        lambda x: f'<a href="{x}" target="_blank">{unquote(x, encoding="utf-8")}</a>'
                    )
                    display_df['title'] = display_df.apply(
                        lambda row: f"{row['title']}<br><span class='length-info'>({row['title_length']}æ–‡å­—)</span>", 
                        axis=1
                    )
                    display_df['description'] = display_df.apply(
                        lambda row: f"{row['description']}<br><span class='length-info'>({row['description_length']}æ–‡å­—)</span>", 
                        axis=1
                    )
                    display_df['status'] = display_df.apply(
                        lambda row: f"ã‚¿ã‚¤ãƒˆãƒ«: <span class='{get_status_class(row['title_status'])}'>{row['title_status']}</span><br>" +
                                  f"ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: <span class='{get_status_class(row['description_status'])}'>{row['description_status']}</span>",
                        axis=1
                    )

                    # è¡¨ç¤ºã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ
                    st.write(display_df[['url', 'title', 'description', 'status']].to_html(
                        escape=False, index=False), unsafe_allow_html=True)

                with tab2:
                    st.subheader("ğŸ“‘ è¦‹å‡ºã—æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯")
                    # HTMLã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¯èƒ½ã«ã™ã‚‹
                    st.markdown("""
                        <style>
                        .dataframe td {
                            max-width: 300px;
                            white-space: normal !important;
                            padding: 8px !important;
                            vertical-align: top;
                            word-break: break-all;
                        }
                        .dataframe th {
                            padding: 8px !important;
                            background-color: #f8f9fa;
                        }
                        </style>
                    """, unsafe_allow_html=True)
                    # URLã‚«ãƒ©ãƒ ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
                    display_df2 = df.copy()
                    display_df2['url'] = display_df2['url'].apply(
                        lambda x: f'<a href="{x}" target="_blank">{unquote(x, encoding="utf-8")}</a>'
                    )
                    st.write(display_df2[['url', 'heading_issues']].to_html(escape=False, index=False), unsafe_allow_html=True)
                
                with tab3:
                    st.subheader("ğŸ”¤ è‹±èªã®ã¿ã®è¦‹å‡ºã—ã®ãƒã‚§ãƒƒã‚¯")
                    # URLã‚«ãƒ©ãƒ ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
                    display_df3 = df.copy()
                    display_df3['url'] = display_df3['url'].apply(
                        lambda x: f'<a href="{x}" target="_blank">{unquote(x, encoding="utf-8")}</a>'
                    )
                    st.write(display_df3[['url', 'english_only_headings']].to_html(escape=False, index=False), unsafe_allow_html=True)
                
                with tab4:
                    st.subheader("ğŸ–¼ï¸ altå±æ€§ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ç”»åƒ")
                    # DataFrameã®è¡¨ç¤ºå‰ã«HTMLã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¯èƒ½ã«ã™ã‚‹
                    st.markdown("""
                        <style>
                        .dataframe a {
                            color: #1E88E5;
                            text-decoration: underline;
                            word-break: break-all;
                        }
                        .dataframe td {
                            max-width: 300px;
                            white-space: normal !important;
                            padding: 8px !important;
                            vertical-align: top;
                            word-break: break-all;
                        }
                        .dataframe th {
                            padding: 8px !important;
                            background-color: #f8f9fa;
                        }
                        .image-url {
                            display: block;
                            margin: 5px 0;
                            word-break: break-all;
                        }
                        </style>
                    """, unsafe_allow_html=True)
                    # URLã‚«ãƒ©ãƒ ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
                    display_df4 = df.copy()
                    display_df4['url'] = display_df4['url'].apply(
                        lambda x: f'<a href="{x}" target="_blank">{unquote(x, encoding="utf-8")}</a>'
                    )
                    st.write(display_df4[['url', 'images_without_alt']].to_html(escape=False, index=False), unsafe_allow_html=True)
                
                with tab5:
                    st.subheader("ğŸ”§ HTMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯")
                    # URLã‚«ãƒ©ãƒ ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
                    display_df5 = df.copy()
                    display_df5['url'] = display_df5['url'].apply(
                        lambda x: f'<a href="{x}" target="_blank">{unquote(x, encoding="utf-8")}</a>'
                    )
                    st.write(display_df5[['url', 'html_syntax']].to_html(escape=False, index=False), unsafe_allow_html=True)
            else:
                st.error("ãƒã‚§ãƒƒã‚¯å¯èƒ½ãªãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def get_status_class(status):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸCSSã‚¯ãƒ©ã‚¹ã‚’è¿”ã™"""
    if 'âœ…' in status or 'OK' in status:
        return 'status-ok'
    return 'status-error'

if __name__ == "__main__":
    main() 